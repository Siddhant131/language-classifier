{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy (SVM): 0.9949\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_and_preprocess_data(train):\n",
    "    data = pd.read_csv(train)\n",
    "    \n",
    "    data['cleaned_text'] = data['text'].apply(lambda x: x.lower())\n",
    "    data['cleaned_text'] = data['cleaned_text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "data_identify = load_and_preprocess_data(\"train_updated.csv\")\n",
    "x_identify = data_identify[\"cleaned_text\"]\n",
    "y_identify = data_identify[\"labels\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_identify_encoded = label_encoder.fit_transform(y_identify)\n",
    "\n",
    "# Feature extraction\n",
    "# Use TF-IDF with word and character n-grams\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),  \n",
    "    max_features=10000,\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 5),  \n",
    "    max_features=10000,\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X_word = tfidf_word.fit_transform(x_identify)\n",
    "X_char = tfidf_char.fit_transform(x_identify)\n",
    "\n",
    "X_combined = np.hstack((X_word.toarray(), X_char.toarray()))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_combined, y_identify_encoded, test_size=0.2, random_state=42, stratify=y_identify_encoded\n",
    ")\n",
    "svm_model = LinearSVC(C=1.0, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_preds = svm_model.predict(X_val)\n",
    "print(f\"Validation accuracy (SVM): {accuracy_score(y_val, svm_preds):.4f}\")\n",
    "\n",
    "final_model = svm_model\n",
    "\n",
    "# Re-train on full training data\n",
    "final_model.fit(X_combined, y_identify_encoded)\n",
    "\n",
    "# Load and evaluate on test data\n",
    "data_identify_test = load_and_preprocess_data(\"test_updated.csv\")\n",
    "x_identify_test = data_identify_test[\"cleaned_text\"]\n",
    "y_identify_test = data_identify_test[\"labels\"]\n",
    "y_identify_test_encoded = label_encoder.transform(y_identify_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Identification Model Accuracy: 0.9931\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      1.00      1.00       500\n",
      "   Bulgarian       1.00      1.00      1.00       500\n",
      "     Chinese       0.96      1.00      0.98       500\n",
      "       Dutch       1.00      0.99      0.99       500\n",
      "     English       1.00      1.00      1.00       500\n",
      "      French       1.00      1.00      1.00       500\n",
      "      German       1.00      1.00      1.00       500\n",
      "       Greek       1.00      1.00      1.00       500\n",
      "       Hindi       1.00      0.97      0.98       500\n",
      "     Italian       1.00      0.99      0.99       500\n",
      "    Japanese       1.00      1.00      1.00       500\n",
      "      Polish       1.00      1.00      1.00       500\n",
      "  Portuguese       0.99      0.99      0.99       500\n",
      "     Russian       1.00      1.00      1.00       500\n",
      "     Spanish       1.00      1.00      1.00       500\n",
      "     Swahili       0.97      1.00      0.98       500\n",
      "        Thai       1.00      0.98      0.99       500\n",
      "     Turkish       0.97      1.00      0.99       500\n",
      "        Urdu       1.00      0.96      0.98       500\n",
      "  Vietnamese       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Identified Language: Hindi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform test data\n",
    "X_test_word = tfidf_word.transform(x_identify_test)\n",
    "X_test_char = tfidf_char.transform(x_identify_test)\n",
    "X_test_combined = np.hstack((X_test_word.toarray(), X_test_char.toarray()))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds = final_model.predict(X_test_combined)\n",
    "accuracy_identify = accuracy_score(y_identify_test_encoded, test_preds)\n",
    "print(f'Language Identification Model Accuracy: {accuracy_identify:.4f}')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_identify_test_encoded, \n",
    "    test_preds,\n",
    "    target_names=label_encoder.classes_\n",
    "))\n",
    "\n",
    "# Create function for language identification\n",
    "def identify_language(text):\n",
    "    # Preprocess input text\n",
    "    cleaned_text = text.lower()\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Transform using both vectorizers\n",
    "    text_word_features = tfidf_word.transform([cleaned_text])\n",
    "    text_char_features = tfidf_char.transform([cleaned_text])\n",
    "    \n",
    "    # Combine features\n",
    "    text_combined = np.hstack((text_word_features.toarray(), text_char_features.toarray()))\n",
    "    \n",
    "    # Predict\n",
    "    predicted_label = final_model.predict(text_combined)\n",
    "    predicted_language = label_encoder.inverse_transform(predicted_label)\n",
    "    confidence = np.max(final_model.predict_proba(text_combined)) if hasattr(final_model, 'predict_proba') else None\n",
    "    \n",
    "    return predicted_language[0], confidence\n",
    "\n",
    "# Interactive language identification\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    user_input_identify = input(\"\\nEnter a text for language identification: \")\n",
    "        \n",
    "    predicted_lang, confidence = identify_language(user_input_identify)\n",
    "    print(f'Identified Language: {predicted_lang}')\n",
    "    if confidence:\n",
    "        print(f'Confidence: {confidence:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
